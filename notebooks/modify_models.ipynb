{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = pathlib.Path.cwd()\n",
    "edited_components_dir = os.path.join(filepath, \"..\", \"edited_components\")\n",
    "\n",
    "if not os.path.exists(edited_components_dir):\n",
    "    os.makedirs(edited_components_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO\n",
    "\n",
    "For our object detection we are using YOLOv8. We are actually using 2 seperate models for person and face detectinon. To apply the unified backbone into the YOLOv8 model branches, we need to modify the YOLOv8 model to accept the backbone's output as an input. To do that, we need to remove the image input layers from the YOLOv8 model and replace it with the backbone's output. We will work on the backbone's output later, for now, we neeed simply need to remove the image input layers from the YOLOv8 model and add a small adapter layer to replace it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = pathlib.Path.cwd()\n",
    "\n",
    "model_path = os.path.join(filepath, \"..\", \"component_models\", \"yolov8n.pt\")\n",
    "yolo_model = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(yolo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out YOLO shows that the first two layers are for augmenting the image to be processed. We can remove these two layers and use the rest of the network to process the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomYOLO(nn.Module):\n",
    "    def __init__(self, yolo_model, backbone_channels=2048):\n",
    "        super(CustomYOLO, self).__init__()\n",
    "\n",
    "        # Gradual channel reduction with normalization and activation\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Conv2d(backbone_channels, 512, kernel_size=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 128, kernel_size=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 32, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        # Remove the first two layers of YOLO, which are for processing input\n",
    "        self.yolo = nn.Sequential(*list(yolo_model.model.model)[2:])\n",
    "        \n",
    "    def forward(self, backbone_features):\n",
    "        x = self.adapter(backbone_features)\n",
    "        x = self.yolo(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, backbone_features):\n",
    "        x = self.adapter(backbone_features)\n",
    "        detections = self.yolo(x)\n",
    "        \n",
    "        # if self.training:\n",
    "        return detections\n",
    "        \n",
    "        # TODO: If inference, process detections\n",
    "        # processed_detections = []\n",
    "        # for detection in detections:\n",
    "        #     processed = self._process_detection(detection)\n",
    "        #     processed_detections.append(processed)\n",
    "        # return processed_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(filepath, \"..\", \"component_models\", \"yolov8n.pt\")\n",
    "yolo_model = YOLO(model_path)\n",
    "person_detect_branch = CustomYOLO(yolo_model)\n",
    "torch.save(person_detect_branch, os.path.join(filepath, \"..\", \"edited_components\", \"custom_yolo.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(filepath, \"..\", \"component_models\", \"yolov8n-face.pt\")\n",
    "yolo_face_model = YOLO(model_path)\n",
    "face_detect_branch = CustomYOLO(yolo_face_model)\n",
    "torch.save(face_detect_branch, os.path.join(filepath, \"..\", \"edited_components\", \"custom_yolo_face.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaFace\n",
    "\n",
    "To recognize faces we are using AdaFace. This is a similar process to YOLO, where we need to remove the image input layers from the model and replace it with the backbone's output. To do this, we have copied the AdaFace model PyTorch code and modified it to accept the backbone's output as an input. We apply the same forward as the original code to keep the same functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', 'libs'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import net_adaface\n",
    "from net_adaface import build_model\n",
    "from head_adaface import build_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaFace has stored the weights of the model as a state dict. Due to this, we need to load the weights of the model into the original model first and then transfer the weights to the modified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdaFace(nn.Module):\n",
    "    def __init__(self, pretrained_path, config):\n",
    "        super(CustomAdaFace, self).__init__()\n",
    "        \n",
    "        # Load the complete pre-trained AdaFace model\n",
    "        self.adaface_model = build_model(model_name=config.arch)\n",
    "        \n",
    "        checkpoint = torch.load(pretrained_path)\n",
    "        if 'state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['state_dict']\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "\n",
    "        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "        \n",
    "        \n",
    "        # Load weights except for input layer\n",
    "        filtered_state_dict = {k: v for k, v in state_dict.items() \n",
    "                             if not k.startswith('input_layer')}\n",
    "        self.adaface_model.load_state_dict(filtered_state_dict, strict=False)\n",
    "        \n",
    "        # Replace input layer with an adapter that takes backbone features\n",
    "        self.adaface_model.input_layer = nn.Sequential(\n",
    "            nn.Conv2d(2048, 64, kernel_size=1),  # First reduce channels\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(64),  # AdaFace uses PReLU\n",
    "            # Additional layers to match the processing of original input_layer\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(64)\n",
    "        )\n",
    "        \n",
    "        # Original AdaFace head\n",
    "        self.head = build_head(\n",
    "            head_type=config.head,\n",
    "            embedding_size=512,\n",
    "            class_num=config.num_classes,\n",
    "            m=config.m,\n",
    "            h=config.h,\n",
    "            t_alpha=config.t_alpha,\n",
    "            s=config.s\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels=None):\n",
    "        # Forward through modified AdaFace model\n",
    "        embeddings, norms = self.adaface_model(x)\n",
    "        \n",
    "        if labels is not None:\n",
    "            return self.head(embeddings, norms, labels)\n",
    "        return embeddings, norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.arch = 'ir_50'\n",
    "        self.head = 'adaface'\n",
    "        self.num_classes = 1000  # Update this based on your dataset\n",
    "        self.m = 0.4\n",
    "        self.h = 0.333\n",
    "        self.t_alpha = 0.01\n",
    "        self.s = 64.0\n",
    "\n",
    "config = Config()\n",
    "\n",
    "ada_face_model_path = os.path.join(filepath, \"..\", \"component_models\", \"adaface_ir50_ms1mv2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/git_projects/Person-Recognition-for-Pose-Estimation/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\AdaFace with the following property\n",
      "self.m 0.4\n",
      "self.h 0.333\n",
      "self.s 64.0\n",
      "self.t_alpha 0.01\n"
     ]
    }
   ],
   "source": [
    "face_rec_branch = CustomAdaFace(ada_face_model_path, config)\n",
    "torch.save(face_rec_branch, os.path.join(filepath, \"..\", \"edited_components\", \"custom_ada_face.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Pose\n",
    "\n",
    "ViT Pose is a model that detects human poses. The model is avalible on Hugging Face in the transformers library. This is very useful, but we need to take a slightly different approach to use it. Instead of copying over the model to a custom version, we instead create a new model that simply has the adaper layer followed by the ViT Pose model. We then edit the patch embedding layer to an identity matrix to simply skip the layer. This makes it a much simpler process than the previous two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, VitPoseForPoseEstimation\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVitPose(nn.Module):\n",
    "    def __init__(self, vit_pose_model: VitPoseForPoseEstimation, backbone_channels=2048):\n",
    "        super(CustomVitPose, self).__init__()\n",
    "        \n",
    "        # Get the hidden size from the model's config\n",
    "        hidden_size = vit_pose_model.backbone.config.hidden_size\n",
    "        \n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Conv2d(backbone_channels, hidden_size, kernel_size=1),\n",
    "            nn.LayerNorm([hidden_size]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.LayerNorm([hidden_size]),\n",
    "        )\n",
    "        \n",
    "        self.vit_pose = vit_pose_model\n",
    "        self.vit_pose.backbone.embeddings.patch_embeddings = nn.Identity()\n",
    "\n",
    "    def forward(self, backbone_features):\n",
    "        x = self.adapter(backbone_features)\n",
    "        return self.vit_pose(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_vit_pose_model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device)\n",
    "pose_detect_branch = CustomVitPose(hf_vit_pose_model)\n",
    "torch.save(pose_detect_branch, os.path.join(filepath, \"..\", \"edited_components\", \"custom_vit_pose.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone\n",
    "\n",
    "For our backbone we chose Resnet50. We chose Resnet50 because it is a well known and well tested backbone that has been used in many object detection models. This class copies the Resnet50 model format, exept for the last layer. Instead of a classifyer output, we have branches depending on the task. The task is passed into the forward funciton, and only the relevant branch's connection is returned. The full multitask branch is not processed at this stage, but due to the seperate architectures between YOLOv8, AdaFace, and ViT Pose, we need the backbone to have seperate outputs to properly provide valid inputs to each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to downloading the backbone via `download_models.ipynb`, we can instead take the simpler approach of using the `torchvision` library to download the model. This is done by calling `torchvision.models.resnet50(pretrained=True)`. This will download the model and load the pretrained weights. We can then modify the model to have the desired outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/git_projects/Person-Recognition-for-Pose-Estimation/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/william/git_projects/Person-Recognition-for-Pose-Estimation/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet_model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(resnet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, we copy the structure of Resnet50, but modify the last layer to have inputs for the individual branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(MultiTaskResNetFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Just the core ResNet layers\n",
    "        self.conv1 = original_model.conv1\n",
    "        self.bn1 = original_model.bn1\n",
    "        self.relu = original_model.relu\n",
    "        self.maxpool = original_model.maxpool\n",
    "        self.layer1 = original_model.layer1\n",
    "        self.layer2 = original_model.layer2\n",
    "        self.layer3 = original_model.layer3\n",
    "        self.layer4 = original_model.layer4\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Basic feature extraction\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x  # Output shape: [batch_size, 2048, H/32, W/32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = MultiTaskResNetFeatureExtractor(resnet_model)\n",
    "torch.save(feature_extractor, os.path.join(filepath, \"..\", \"edited_components\", \"resnet_feature_extractor.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Model\n",
    "\n",
    "Now that we have all of the branches and the backbone of the model, we can now combine them all into a single multi-task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, backbone, yolo_face, yolo_person, ada_face, vit_pose):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        if any(model_args is None for model_args in [backbone, yolo_face, yolo_person, ada_face, vit_pose]):\n",
    "            raise ValueError(\"All models must be provided\")\n",
    "\n",
    "        self.current_task = 'person_detection'\n",
    "        self.backbone = backbone\n",
    "        self.yolo_face = yolo_face\n",
    "        self.yolo_person = yolo_person\n",
    "        self.ada_face = ada_face\n",
    "        self.vit_pose = vit_pose\n",
    "\n",
    "    def set_task(self, task_name):\n",
    "        supported_tasks = ['face_detection', 'person_detection', 'pose_estimation', 'face_identification']\n",
    "        if task_name not in supported_tasks:\n",
    "            raise ValueError(f\"Task {task_name} not supported. Available tasks: {', '.join(supported_tasks)}\")\n",
    "        self.current_task = task_name\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get features from backbone for the task\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Use the correct branch for the inputted task\n",
    "        # torch.where is used here for efficiency\n",
    "        return torch.where(self.current_task == 'pose_estimation',\n",
    "            self.vit_pose(features),\n",
    "            torch.where(self.current_task == 'person_detection',\n",
    "                self.yolo_person(features),\n",
    "                torch.where(self.current_task == 'face_detection',\n",
    "                    self.yolo_face(features),\n",
    "                    self.ada_face(features)\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = CombinedModel(feature_extractor, face_detect_branch, person_detect_branch, face_rec_branch, pose_detect_branch)\n",
    "torch.save(combined_model, os.path.join(filepath, \"..\", \"edited_components\", \"combined_model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
